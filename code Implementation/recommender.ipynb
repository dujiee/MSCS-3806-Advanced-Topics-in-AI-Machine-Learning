{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "# disable tensorflow warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# use tensorflow v1\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "class RBM(object):\n",
    "    def __init__(self, num_v, id, num_h, batch_size, learning_rate,\n",
    "                 num_epoch, k=2):\n",
    "        self.num_v = num_v\n",
    "        self.num_h = num_h\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epoch = num_epoch\n",
    "        self.k = k\n",
    "        self.id = id\n",
    "        self.W, self.a, self.b = self._init_parameter(id)\n",
    "\n",
    "    def _init_parameter(self, id):\n",
    "        abs_val = np.sqrt(2.0 / (self.num_h + self.num_v))\n",
    "        with tf.variable_scope('rbm{}_parameter'.format(id)):\n",
    "            W = tf.get_variable('weights', shape=(self.num_v, self.num_h),\n",
    "                                initializer=tf.random_uniform_initializer(\n",
    "                minval=-abs_val, maxval=abs_val))\n",
    "            a = tf.get_variable('visible_bias', shape=(self.num_v),\n",
    "                                initializer=tf.zeros_initializer())\n",
    "            b = tf.get_variable('hidden_bias', shape=(self.num_h),\n",
    "                                initializer=tf.zeros_initializer())\n",
    "        return W, a, b\n",
    "\n",
    "    def _gibbs_sampling(self, v):\n",
    "        v0 = v\n",
    "        prob_h_v0 = self._prob_h_given_v(v0)\n",
    "        vk = v\n",
    "        prob_h_vk = prob_h_v0\n",
    "        for _ in range(self.k):\n",
    "            hk = self._bernoulli_sampling(prob_h_vk)\n",
    "            prob_v_hk = self._prob_v_given_h(hk)\n",
    "            vk_tmp = prob_v_hk\n",
    "            vk = tf.where(tf.equal(v0, 0.0), v0, vk_tmp)\n",
    "            prob_h_vk = self._prob_h_given_v(vk)\n",
    "        return v0, prob_h_v0, vk, prob_h_vk\n",
    "\n",
    "    def _prob_v_given_h(self, h):\n",
    "        return tf.sigmoid(\n",
    "            tf.add(self.a, tf.matmul(h, tf.transpose(self.W))))\n",
    "\n",
    "    def _prob_h_given_v(self, v):\n",
    "        return tf.sigmoid(tf.add(self.b, tf.matmul(v, self.W)))\n",
    "\n",
    "    def _bernoulli_sampling(self, prob):\n",
    "        distribution = tf.distributions.Bernoulli(\n",
    "            probs=prob, dtype=tf.float32)\n",
    "        return tf.cast(distribution.sample(), tf.float32)\n",
    "\n",
    "    def _compute_gradients(self, v0, prob_h_v0, vk, prob_h_vk):\n",
    "        outer_product0 = tf.matmul(tf.transpose(v0), prob_h_v0)\n",
    "        outer_productk = tf.matmul(tf.transpose(vk), prob_h_vk)\n",
    "        W_grad = tf.reduce_mean(outer_product0 - outer_productk, axis=0)\n",
    "        a_grad = tf.reduce_mean(v0 - vk, axis=0)\n",
    "        b_grad = tf.reduce_mean(prob_h_v0 - prob_h_vk, axis=0)\n",
    "        return W_grad, a_grad, b_grad\n",
    "\n",
    "    def _optimize(self, v):\n",
    "        v0, prob_h_v0, vk, prob_h_vk = self._gibbs_sampling(v)\n",
    "        W_grad, a_grad, b_grad = self._compute_gradients(\n",
    "            v0, prob_h_v0, vk, prob_h_vk)\n",
    "        para_update = [tf.assign(self.W, tf.add(self.W, self.learning_rate*W_grad)),\n",
    "                       tf.assign(self.a, tf.add(\n",
    "                           self.a, self.learning_rate*a_grad)),\n",
    "                       tf.assign(self.b, tf.add(self.b, self.learning_rate*b_grad))]\n",
    "        bool_mask = tf.cast(tf.where(tf.equal(v0, 0.0), x=tf.zeros_like(\n",
    "            v0), y=tf.ones_like(v0)), dtype=tf.bool)\n",
    "        # mask the zero values because they are not included in the error calculation\n",
    "        v0_mask = tf.boolean_mask(v0, bool_mask)\n",
    "        vk_mask = tf.boolean_mask(vk, bool_mask)\n",
    "        error = tf.metrics.mean_squared_error(v0_mask, vk_mask)[1]\n",
    "        return para_update, error\n",
    "\n",
    "    def train(self, X_train):\n",
    "        X_train_plac = tf.placeholder(tf.float32, [None, self.num_v])\n",
    "        para_update, error = self._optimize(X_train_plac)\n",
    "        init = tf.group(tf.global_variables_initializer(),\n",
    "                        tf.local_variables_initializer())\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            epochs_err = []\n",
    "            n_batch = int(X_train.shape[0] / self.batch_size)\n",
    "            for epoch in range(1, self.num_epoch + 1):\n",
    "                epoch_err_sum = 0\n",
    "                for batch_number in range(n_batch):\n",
    "                    batch = X_train[batch_number * self.batch_size:\n",
    "                                    (batch_number + 1) * self.batch_size]\n",
    "                    parameters, batch_err = sess.run((para_update, error), feed_dict={\n",
    "                        X_train_plac: batch})\n",
    "                    epoch_err_sum += batch_err\n",
    "                epochs_err.append(epoch_err_sum / n_batch)\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Training error at epoch %s: %s\" %\n",
    "                          (epoch, epochs_err[-1]))\n",
    "        return parameters\n",
    "\n",
    "    def predict(self, v, parameters):\n",
    "        W, a, b = parameters\n",
    "        prob_h_v = 1 / (1 + np.exp(-(b + np.matmul(v, W))))\n",
    "        h = np.random.binomial(1, p=prob_h_v)\n",
    "        prob_v_h = 1 / (1 + np.exp(-(a + np.matmul(h, np.transpose(W)))))\n",
    "        return prob_v_h\n",
    "\n",
    "    def hidden_layer(self, v, parameters):\n",
    "        W, a, b = parameters\n",
    "        h = 1 / (1 + np.exp(-(b + np.matmul(v, W))))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(object):\n",
    "    def __init__(self, layer_sizes, batch_size, learning_rates, num_epoch, k=2):\n",
    "        self.rbms = []\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            rbm = RBM(num_v=layer_sizes[i-1], id=i, num_h=layer_sizes[i], batch_size=batch_size,\n",
    "                      learning_rate=learning_rates[i-1], num_epoch=num_epoch, k=k)\n",
    "            self.rbms.append(rbm)\n",
    "\n",
    "    def train(self, X_train):\n",
    "        self.rbms_para = []\n",
    "        input_data = None\n",
    "        for rbm in self.rbms:\n",
    "            if input_data is None:\n",
    "                input_data = X_train.copy()\n",
    "            parameters = rbm.train(input_data)\n",
    "            self.rbms_para.append(parameters)\n",
    "            input_data = rbm.hidden_layer(input_data, parameters)\n",
    "\n",
    "    def predict(self, X):\n",
    "        data = None\n",
    "        for rbm, parameters in zip(self.rbms, self.rbms_para):\n",
    "            if data is None:\n",
    "                data = X.copy()\n",
    "            # fix for one layer DBN\n",
    "            if len(self.rbms) == 1 and rbm.id == len(self.rbms):\n",
    "                return rbm.predict(data, parameters)\n",
    "            data = rbm.hidden_layer(data, parameters)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0 stars: 21384032\n",
      "Number of 1 stars: 56174\n",
      "Number of 2 stars: 107557\n",
      "Number of 3 stars: 261197\n",
      "Number of 4 stars: 348971\n",
      "Number of 5 stars: 226309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_path = '../ml-1m/ratings.dat'\n",
    "num_users = 6040\n",
    "num_movies = 3706\n",
    "data = np.zeros([num_users, num_movies], dtype=np.float32)\n",
    "movie_dict = {}\n",
    "with open(data_path, 'r') as file:\n",
    "    for line in file.readlines()[1:]:\n",
    "        user_id, movie_id, rating, _ = line.split(\"::\")\n",
    "        user_id = int(user_id) - 1\n",
    "        if movie_id not in movie_dict:\n",
    "            movie_dict[movie_id] = len(movie_dict)\n",
    "        rating = float(rating) / 5\n",
    "        data[user_id, movie_dict[movie_id]] = rating\n",
    "data = np.reshape(data, [data.shape[0], -1])\n",
    "values, counts = np.unique(data, return_counts=True)\n",
    "for value, count in zip(values, counts):\n",
    "    print(f'Number of {int(value*5)} stars: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 01:07:00.486969: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error at epoch 10: 0.04473554427247672\n",
      "Training error at epoch 20: 0.04221944697201252\n",
      "Training error at epoch 30: 0.04116101199317546\n",
      "Training error at epoch 40: 0.040564773249484244\n",
      "Training error at epoch 50: 0.04017959224681059\n",
      "Training error at epoch 60: 0.0399079890034738\n",
      "Training error at epoch 70: 0.03970624453255108\n",
      "Training error at epoch 80: 0.03954996665318807\n",
      "Training error at epoch 90: 0.03942516335241851\n",
      "Training error at epoch 100: 0.03932305851152965\n",
      "0.03795511475475648\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "np.random.shuffle(data)\n",
    "num_train = int(0.9 * data.shape[0])\n",
    "data_train, data_test = data[:num_train, :], data[num_train:, :]\n",
    "sim_index = np.zeros_like(data_test, dtype=bool)\n",
    "perc_sim = 0.2\n",
    "for i, user_test in enumerate(data_test):\n",
    "    exist_index = np.where(user_test > 0.0)[0]\n",
    "    sim_index[i, np.random.choice(exist_index,\n",
    "                                  int(len(exist_index)*perc_sim))] = True\n",
    "\n",
    "\n",
    "dbn = DBN(layer_sizes=[num_movies, 80], batch_size=64, num_epoch=100, learning_rates=[0.1], k=5)\n",
    "dbn.train(data_train)\n",
    "data_test_sim = np.copy(data_test)\n",
    "data_test_sim[sim_index] = 0.0\n",
    "\n",
    "prediction = dbn.predict(data_test_sim)\n",
    "print(mean_squared_error(data_test[sim_index], prediction[sim_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shawshank Redemption, The (1994)', 'Shall We Dance? (1937)', 'Patton (1970)', '42 Up (1998)', 'Killing Fields, The (1984)']\n",
      "['Usual Suspects, The (1995)', 'Godfather, The (1972)', 'Godfather: Part II, The (1974)', 'M (1931)', 'Double Indemnity (1944)']\n",
      "['Sixth Sense, The (1999)', 'American Beauty (1999)', 'Matrix, The (1999)', 'Tarzan (1999)', 'Girl, Interrupted (1999)']\n",
      "[\"Ferris Bueller's Day Off (1986)\", 'Sound of Music, The (1965)', 'Airplane! (1980)', 'Tarzan (1999)', 'Bambi (1942)']\n"
     ]
    }
   ],
   "source": [
    "# Movie recommendation\n",
    "# The recommender receive the user inputs including User ID, Movie ID, Rating: a rating from 1-5 stars for a specific movie\n",
    "# after that, the model should give out the recommendation of the movies that the user might like.\n",
    "# Besides, the user can give some filter to the recommendation, such as the genre of the movie, the year of the movie, etc.\n",
    "\n",
    "# load movie data\n",
    "movie_path = '../ml-1m/movies.dat'\n",
    "movie_data = {}\n",
    "\n",
    "with open(movie_path, 'r', encoding='latin-1') as file:\n",
    "    for line in file.readlines():\n",
    "        line = line.strip()\n",
    "        movie_id, title, genre = line.split(\"::\")\n",
    "        year = title[-5:-1] if title[-1] == ')' else None\n",
    "        movie_data[movie_id] = {\n",
    "            'title': title,\n",
    "            'genre': genre.split('|'),\n",
    "            'year': year\n",
    "        }\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class Recommender:\n",
    "    def __init__(self):\n",
    "        self.rating = np.zeros([num_movies], dtype=np.float32)\n",
    "\n",
    "    def input_rating(self, movie_id, rating):\n",
    "        self.rating[movie_dict[movie_id]] = float(rating) / 5\n",
    "\n",
    "    def get_recommendation(self, query=None):\n",
    "        index_movie = {value: key for key, value in movie_dict.items()}\n",
    "        prediction = dbn.predict(np.reshape(self.rating, [1, -1]))\n",
    "        watched = np.where(self.rating != 0)[0]\n",
    "        low_index = np.where(prediction[0] >= 0.6)[0] # 3 to 4 stars\n",
    "        med_index = np.where(prediction[0] >= 0.8)[0] # 4 to 4.5 stars\n",
    "        high_index = np.where(prediction[0] >= 0.9)[0] # 4.5 to 5 stars\n",
    "        # print('Movies watched:',', '.join(movie_data[index_movie[index]]['title'] for index in watched))\n",
    "        # print('Movies with low prediction:', ', '.join(movie_data[index_movie[index]]['title'] for index in low_index if index not in watched and index not in med_index and index not in high_index))\n",
    "        # print('Movies with med prediction:', ', '.join(movie_data[index_movie[index]]['title'] for index in med_index if index not in watched and index not in high_index))\n",
    "        # print('Movies with high prediction:', ', '.join(movie_data[index_movie[index]]['title'] for index in high_index if index not in watched))\n",
    "        \n",
    "        set_watched = set(watched)\n",
    "        low_index = set(low_index) - set_watched\n",
    "        med_index = set(med_index) - set_watched\n",
    "        high_index = set(high_index) - set_watched\n",
    "        recommendation = deque()\n",
    "        for i in high_index:\n",
    "            recommendation.append(i)\n",
    "        for i in med_index:\n",
    "            recommendation.append(i)\n",
    "        for i in low_index:\n",
    "            recommendation.append(i)\n",
    "        \n",
    "        # filter the recommendation\n",
    "        if not query:\n",
    "            final_recommendation = []\n",
    "            count = 5\n",
    "            while count > 0 and recommendation:\n",
    "                final_recommendation.append(recommendation.popleft())\n",
    "                count -= 1\n",
    "        else:\n",
    "            final_recommendation = []\n",
    "            count = 5\n",
    "            while count > 0 and recommendation:\n",
    "                idx = recommendation.popleft()\n",
    "                # print(movie_data[index_movie[idx]])\n",
    "                if 'genre' in query and query['genre'] not in movie_data[index_movie[idx]]['genre']:\n",
    "                    continue\n",
    "                if 'year' in query and query['year'] != movie_data[index_movie[idx]]['year']:\n",
    "                    continue\n",
    "                if 'rating' in query and prediction[0][idx]*5 >= query['rating'] and prediction[0][idx]*5 < query['rating']+1:\n",
    "                    continue\n",
    "                final_recommendation.append(idx)\n",
    "                count -= 1\n",
    "        \n",
    "        return [movie_data[index_movie[index]]['title'] for index in final_recommendation]\n",
    "\n",
    "\n",
    "recommender = Recommender()\n",
    "user_1_ratings = {'1193': 5, \n",
    "                '661': 3,\n",
    "                '914': 3,\n",
    "                '3408': 4,\n",
    "                '2355': 5,\n",
    "                '1197': 3,\n",
    "                '1287': 5,\n",
    "                '2804': 5,\n",
    "                '594': 4,\n",
    "                '919': 4,\n",
    "                '595': 5,\n",
    "                '938': 4,\n",
    "                '2398': 4,\n",
    "                }\n",
    "for movie_id, rating in user_1_ratings.items():\n",
    "    recommender.input_rating(movie_id, rating)\n",
    "print(recommender.get_recommendation())\n",
    "print(recommender.get_recommendation(query={'genre': 'Crime'}))\n",
    "print(recommender.get_recommendation(query={'year': '1999'}))\n",
    "print(recommender.get_recommendation(query={'rating': 4}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
